{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals\n",
    "\n",
    "- in an experiment, we cant observe the variables' true values directly\n",
    "  - we observe other values\n",
    "  - we make assumptions as to how they are distributed\n",
    "  - we can estimate the true value\n",
    "  - law of large numbers: when our sample is big enough, the sample parameters approach the population parameters\n",
    "- with continuous values, it's useless to say that the mean is equal to a certain value (why?)\n",
    "- **confidence interval** - a range of values that we're fairly sure contains the true value\n",
    "  - how confident? A matter of choice\n",
    "- **confidence level** - the probability that the value falls within the interval "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals - Interpretation\n",
    "\n",
    "- similar to the probability interpretations\n",
    "- to illustrate these, let's take a confidence interval [5; 7,3] and a 70% confidence level\n",
    "- frequency\n",
    "  - if we perform the experiment many times, 70% of the values will fall in the interval [5; 7,3] and 30% - outside it\n",
    "- certainty of next trial\n",
    "  - next time we perform the experiment, we are 70% certain that the value will fall within [5; 7,3]\n",
    "  - note that this is a statement **about the interval**, not about the value\n",
    "- typically used confidence intervals • 50%; 90%; 95%; 99,7% \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals and Z-Scores\n",
    "\n",
    "- observe the Z-distribution (Gaussian, $ \\mu = 0, \\sigma = 1$)\n",
    "- what's the probability that a value drawn from it $x \\in [-2; 1]$?\n",
    "  - this corresponds to the shaded area in the graph\n",
    "  - the cumulative function gives us the area to the left of some value\n",
    "  - shaded area = $cdf(1) — cdf(-2) = 0,819 = 81,9%$\n",
    "- interpretations\n",
    "  - if we draw many random numbers from the Z-distribution, we expect that 81,9% of them will be in [-2; 1]\n",
    "  - if we draw one random number, there is 81,9% chance of it being in [-2; 1]\n",
    "- commonly used intervals\n",
    "  - $1\\sigma \\rightarrow 68,27\\%; 2\\sigma \\rightarrow 95,45\\%; 3\\sigma \\rightarrow 99,73\\% $\n",
    "  - also $1,96\\sigma \\rightarrow 95\\%$\n",
    "![Confidence Z-Scores](confidence-z-scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals Example\n",
    "\n",
    "- note that once again we need to subtract the left white region\n",
    "  - area of shaded region: $p (e.g. p=0,95)$\n",
    "  - area of both tails: $1 - p$\n",
    "  - percentage point of left tail: $\\frac{1-p}{2}$\n",
    "  - percentage point of right tail: $\\frac{1-p}{2} + p = \\frac{1+p}{2}$\n",
    "  \n",
    "```python\n",
    "import scipy.stats as st \n",
    "\n",
    "def get_real_confidence_interyal(probability, mean, std): \n",
    "    lower_area = (1 - probability) / 2 \n",
    "    upper_area = (1 • probability) / 2 \n",
    "    return [ \n",
    "        st.norm.ppf(lower_area, mean, std), \n",
    "        st.norm.ppf(upper_area, mean, std)] \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Hypotheses\n",
    "\n",
    "## Hypotheses\n",
    "\n",
    "- after performing an experiment and getting data, the scientific method requires that we form a hypothesis\n",
    "  - fact, law, theory and hypothesis are different terms\n",
    "- in the simplest case, we have two hypotheses\n",
    "  - null hypothesis ($H_0$) - the status quo is real, \"nothing interesting happens\"\n",
    "  - alternate hypothesis ($H_1$) - what we're trying to demonstrate\n",
    "- types of hypotheses\n",
    "  - attributive - something exists and can be measured\n",
    "  - associative - there is a relationship between two behaviors\n",
    "  - causal - differences in the amount / kind of one behavior cause differences in other behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypotheses - Examples\n",
    "\n",
    "- examples of hypotheses - study of Disneyland visitors\n",
    "  - attributive\n",
    "    - most of the population has heard of Disneyland\n",
    "    - disneyland visitors are diverse in demographics\n",
    "  - associative\n",
    "    - income level is correlated with visiting Disneyland\n",
    "    - people who live closer to Disneyland are more apt to visit Disneyland\n",
    "  - causal\n",
    "    - frequent exposure to Disneyland advertising results in increased attendance\n",
    "    - discounting tickets for local residents produces an increase in visitor numbers\n",
    "- note that attributive hypotheses involve one variable (univariate) while associative and causal hypotheses involve two variables (bivariate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a Hypothesis\n",
    "\n",
    "- in random experiments, we have error sources\n",
    "  - human error, systematic error, random errors, etc.\n",
    "- we cannot prove (or reject) a hypothesis with complete certainty\n",
    "- the errors we can make are two types\n",
    "  - Type I error - reject $H_0$ while it's true (false positive)\n",
    "  - Type II error - accept $H_0$ while $H_1$ is true (false negative)\n",
    "- the possible results can be summarized in the following truth table\n",
    "  - also called confusion matrix\n",
    "![Confusion Matrix](confusion-matrix.png)  \n",
    "\n",
    "- to measure the probability of producing a wrong hypothesis, we use a test statistic - measure of deviations from $H_0$\n",
    "  - different tests produce different measures (statistics)\n",
    "  - we accept or reject the null hypothesis based on the value of the test statistic\n",
    "- let's denote the probability of getting a type I error with $\\alpha$\n",
    "  - each value of the selected test statistic has a corresponding alpha-value\n",
    "  - we perform the experiment, get data and calculate the test statistic value\n",
    "  - from that, we calculate the corresponding alpha-value\n",
    "  - we reject the null hypothesis if $\\alpha < \\alpha_c$, where $\\alpha_c$ is a **critical confidence level**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-test\n",
    "\n",
    "- A Z-test uses the Z-statistic\n",
    "- $H_0$: standard normal distribution\n",
    "- example: light bulb factory\n",
    "  - a factory produces light bulbs with lifetime $X \\thicksim N(\\mu = 500h, \\sigma = 50h)$\n",
    "  - a sample of 25 bulbs has a mean lifetime $ \\bar{x} = 480h$\n",
    "  - is there something wrong with the production line?\n",
    "- forming hypotheses\n",
    "  - $H_0$: The production line works normally, the observed deviation of the sample mean from the population mean is due to chance\n",
    "  - $H_1$: The production line is broken\n",
    "- suppose we take a lot of samples from the entire population\n",
    "  - each sample mean will be different\n",
    "  - the distribution of sample means will be more or less Gaussian\n",
    "    - parameters (our best estimate): $\\mu_{\\bar{x}} = \\mu, \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}$\n",
    "    - here's why the parameters are chosen as such \n",
    "- if $H_0$ is correct, we assume that: $\\bar{x} \\thicksim N (\\mu, \\frac{\\sigma}{\\sqrt{n}})$\n",
    "- Z-statistic\n",
    "  - $Z = \\frac{\\bar{x} - \\mu}{\\sigma_{\\bar{x}}} = \\frac{480 - 500}{50 / \\sqrt{25}} = -2 $\n",
    "- we can see that we are 2 std's below the mean\n",
    "- how extreme is that?\n",
    "  - what's the probability that we get results **as extreme or more extreme** than we observed, assuming the null hypothesis is true?\n",
    "    - less than 5% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-tailed Z-test \n",
    "\n",
    "- we can get the confidence interval from the Z-statistic \n",
    "- we are looking for more extreme values\n",
    "  - values outside the confidence interval\n",
    "  - what's the probability $P(|Z| \\geq 2)$?\n",
    "  - we're looking for a value different than the mean\n",
    "    - we **can't assume** whether it's smaller or larger\n",
    "    - therefore, we have to look at both \"tails\" \n",
    "- if we assume a critical value (also called a p-value) of 5%, the results are significant\n",
    "  - $P(|Z| > 2) \\approx 0,0455 = 4,55\\%$\n",
    "  - we can reject $H_0$ at the 5% level\n",
    "  - even at lower levels, up to 4,55% \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-tailed Z-test\n",
    "\n",
    "- the same logic applies, but now we're looking at one tail only\n",
    "- question: Is the lifespan **significantly lower** than it should be?\n",
    "- cutoff point: $\\alpha_c = 5\\%, Z = —2$\n",
    "  - $P(Z \\leq —2) = \\frac{0,00455}{2} - 0,02275 = 2,275\\% < \\alpha_c$\n",
    "  - answer: Yes, at the given significance level\n",
    "- question: Is the lifespan significantly higher than it should be?\n",
    "  - $P(Z \\geq —2) = 97,725\\% \\alpha_c$\n",
    "  - answer: No, at the given significance level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-test\n",
    "\n",
    "- the Z-test requires that we know the standard deviation of the population \n",
    "  - Usually not available\n",
    "- we can use another test statistic, called **t**\n",
    "- advantages over the Z-test\n",
    "  - we don't need to know the population $\\sigma$\n",
    "  - it's better when we have very small sample sizes (e.g., n < 30)\n",
    "  - it can be used for testing the mean of a sample against a standard, but also for comparing two means\n",
    "    - we can see whether two sets of data are significantly different from each other\n",
    "- null hypothesis: The test statistic follows Student's t-distribution\n",
    "  - similar to Gaussian distribution, with \"fatter\" tails \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Sample t-test\n",
    "\n",
    "- the details of the calculation are fairly complex but we can do this in code\n",
    "  - using scipy.stats\n",
    "- first, we generate 100 random numbers with $ \\mu = 5, \\sigma = 10$\n",
    "- then we ask whether the sample mean is equal to the true mean (and other values, just for testing)\n",
    "- we get the p-value - probability of the null hypothesis being true\n",
    "  - i.e. probability that the mean is equal to the given mean \n",
    "\n",
    "```python\n",
    "sample_data = st.norm.rvs(5, 10, 100) \n",
    "print(st.ttest_lsamp(sample_data, 5).pvalue) # 0.9301 \n",
    "print(st.ttest_lsamp(sample_data, 4).pvalue) # 0.3352 \n",
    "print(st.ttest_lsamp(sample_data, 0).pvalue) # 1.104e-6\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent Two-Sample t-test\n",
    "\n",
    "- we compare two independent distributions\n",
    "  - we want to see whether they have the same mean\n",
    "  - we assume equal variances (scipy can also do tests with unequal variances - important when sample sizes differ)\n",
    "- example: Grain size\n",
    "  - we are given data (in grain_data. csv) of grain sizes from two different farms\n",
    "  - do they differ significantly (at the 95% level)?\n",
    "  - * we can also plot histograms to see what the distributions look like \n",
    "  \n",
    "```python  \n",
    "grain_data = ...\n",
    "st.ttest_ind(grain_data.GreatNorthern, grain_data.BigFour) \n",
    "#Ttest_indResult(statistic=1.312336706487564, \n",
    "# pvalue=0.20792200785311768) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paired Two-Sample t-test\n",
    "\n",
    "- we compare two distributions\n",
    "  - observations in samples can be paired\n",
    "  - examples - before / after observations; comparison between two different treatments applied to the same subjects\n",
    "- example: Drinking water\n",
    "  - we are given data (in water data. csv) of Zn concentration in surface and bottom water at 10 different locations\n",
    "  - does the true average concentration in bottom water exceed that of top water?\n",
    "  - we use a paired t-test because the samples are from the same locations\n",
    "  - it reduces experimental error (and provides stronger evidence)\n",
    "  \n",
    "```python  \n",
    "water_data = ... \n",
    "# We use a one-tailed t-test\n",
    "st.ttest_rel(water_data.surface, water_data.bottom).pvalue / 2 \n",
    "# 0.00044555772891127738 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizations to More Variables\n",
    "\n",
    "- sometimes it's not enough to compare two distributions\n",
    "  - we may want to compare multiple distributions against the same null hypothesis\n",
    "  - e.g. how is the percentage of smokers distributed by income and age?\n",
    "- other times, we create a model and want to evaluate it\n",
    "  - e.g. a linear regression\n",
    "  - we can explain some of the variance in the sample\n",
    "- there are other tests to perform these \"checks\"\n",
    "  - ANOVA (Analysis of Variance) - useful for grouped data\n",
    "    - observe the variance inside groups and between groups\n",
    "  - chi-square(d) test — can be applied to categorical data\n",
    "    - two common types • How good a model is (goodness of fit)\n",
    "    - whether two variables are independent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisys of Variance (ANOVA) \n",
    "\n",
    "- we want to compare several groups\n",
    "- $H_0$: The means of the groups are the same\n",
    "- method (scipy.stats.f oneway())\n",
    "  - for each group $\\Rightarrow$ group mean\n",
    "    - in-group variance: distances from an individual point to the group mean\n",
    "    - between-group variance: distances between the means of two groups\n",
    "  - for the entire data $\\Rightarrow$ total mean (mean of all data)\n",
    "    - also equal to the mean of all group means\n",
    "    - total variance: in-group + between-group\n",
    "- F-statistic (Fisher) variance between groups\n",
    "  - $ F = \\frac{variane between groups}{variance within groups}$\n",
    "  - F - large $\\Rightarrow$ the variance between groups dominates\n",
    "  - for each value of F, there's a corresponding p-value\n",
    "  - if $p \\leq p_c$, we can reject $H_0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-Squared ($x^2$) Test\n",
    "\n",
    "- compares expected (predicted) and observed frequencies\n",
    "  - is there a significant difference between these?\n",
    "  - this is a goodness-of-fit measure\n",
    "    - how well were we able to predict\n",
    "- statistic: $X^2 = \\frac{(f_{observed} - f_{estimated})^2}{f_{estimated}}$\n",
    "- $H_0$: No significant difference between observed and estimated \n",
    "- the test returns the value of the statistic and the p-value corresponding to it\n",
    "- works the same as any other test\n",
    "- python: scipv. stats chisoua re()  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
