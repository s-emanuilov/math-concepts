{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "\n",
    "## Descriptive Statistics \n",
    "\n",
    "- numbers which are used to **summarize** and **describe** data\n",
    "  - we work with all items of interest - **statistical population**\n",
    "  - we don't try to make predictions, just describe what we're seeing\n",
    "- not very useful on their own\n",
    "  - but an important part of other methods\n",
    "- example: pet shop sales\n",
    "  - 100 pets in one month: 40 dogs, 30 cats, 30 other\n",
    "- what percent of all pets are dogs?\n",
    "- what's the mean number of cats sold per month?\n",
    "- we can also represent the information graphically\n",
    "  - what does the distribution of dog sales per day look like?\n",
    "  - what does the cumulative distribution of sales look like?\n",
    "  - how do sales compare? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferential Statistics\n",
    "\n",
    "- in many cases the **population** is too large (or even infinite)\n",
    "  - we represent the population by a subset - **sample**\n",
    "  - the population characteristics can be estimated by using the sample\n",
    "    - we have to be extremely careful how to choose the sample\n",
    "  - in most cases we need **random sampling** of the population\n",
    "- examples\n",
    "  - voting predictions\n",
    "    - we ask a small number of people and we draw inferences about the entire country\n",
    "  - mean salary by age\n",
    "    - we divide people into age groups (e.g. < 20, 20 — 25, 25 — 30, 30 — 35, ...) and ask several people within each age group\n",
    "    - this also makes the continuous variable \"age\" easier to work with \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "- the process of selecting a sample from the population\n",
    "- steps in the sampling process\n",
    "  - define the population\n",
    "  - specify the **sampling frame** - a set of items from the population\n",
    "  - specify the **sampling method** - how to select items from the frame\n",
    "  - determine the sample size\n",
    "  - implement the sampling and collect data\n",
    "- a badly done sampling can induce biases and errors\n",
    "  - selection bias - selecting a non-random sample\n",
    "  - e.g. asking only CEOs of companies when sampling data for salaries by age\n",
    "  - random sampling error - random variations in the results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Methods\n",
    "\n",
    "- non-random sampling\n",
    "  - can be biased\n",
    "  - not representative of the population\n",
    "- random sampling\n",
    "  - every member of the population has equal chance of being chosen\n",
    "  - example: insect population in trees\n",
    "    - trees are numbered 1-200, 10 trees are chosen at random\n",
    "    - all insects are counted on the 10 random trees\n",
    "- stratified sampling\n",
    "  - divide the population into categories (subpopulations)\n",
    "  - for each category, sample at random\n",
    "  - example: foot measurement study —) male / female; age groups\n",
    "  - select samples for each combination { gender; age} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of distributions\n",
    "\n",
    "## Summarizing Distributions\n",
    "\n",
    "- a histogram is a complete description of the sample distribution\n",
    "- we often summarize it using a few descriptive statistics\n",
    "  - **central tendency**\n",
    "    - do the values tend to cluster around a center?\n",
    "  - **modes**\n",
    "    - how many clusters are there? Where are they?\n",
    "  - **variance**\n",
    "    - how much variability is there (how \"spread out\" is the distribution)?\n",
    "  - **tails**\n",
    "    - how quickly do probabilities drop off as we move away from the center(s)?\n",
    "  - **outliers**\n",
    "    - are there extreme values, far from the center(s)?\n",
    "- these are also called **summary statistics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of Central Tendency\n",
    "\n",
    "- **average** - a number which describes a typical data point\n",
    "  - can be calculated in many ways\n",
    "- **arithmetic mean**\n",
    "  - the sum of all measurements divided by the number of observations\n",
    "- **median**\n",
    "  - the middle value of the distribution\n",
    "  - to calculate it, the numbers must be sorted in ascending order\n",
    "  - examples:\n",
    "    - Me({1, 2, 2, 3, 4}) = 2\n",
    "    - Me({1, 2, 2, 3, 4, 10}) = 2,5\n",
    "- **mode**\n",
    "  - the most frequent item\n",
    "  - Mo({1, 3, 2, 3, 4, 3}) = 3\n",
    "  - many \"most frequent items\"> multimodal distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variances\n",
    "\n",
    "- describes how far away a sample is from the sample mean\n",
    "  - all distances from the mean can be positive or negative\n",
    "  - they all sum up to 0 (that's the definition of the mean)\n",
    "  - so we square them to make them positive\n",
    "  - standard deviation: $S(x) = \\sqrt{S^2(x)}$ \n",
    "- in the sample variance formula, there is n —1 in the denominator\n",
    "  - it refers to \"degrees of freedom\" - how many items we can remove\n",
    "    - the number of parameters that can vary\n",
    "  - because all distances sum up to 0, if we know n —1 of them, we can find the last one\n",
    "  - gives us an unbiased estimator \n",
    "- why bother to take the standard deviation?\n",
    "  - instead of using variance directly\n",
    "- its all about units\n",
    "- example:\n",
    "  - let's say we're measuring length in m\n",
    "  - by definition, the variance will have units of ma\n",
    "  - we want to see how far is a certain point from the center and the units don't match\n",
    "    - compare $d = 2m, S^2 = 0,25m^2$ to $d = (2 \\pm 0,5)m$\n",
    "  - in order to make units match, we take the square root\n",
    "  - so we can say \"This measurement is located at 1,5 standard deviations above the mean\"\n",
    "    - in our example, such measurement would be 2,75m\n",
    "    - comparisons like these are very useful in statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population vs. Sample: Measures\n",
    "\n",
    "- there are differences between a population and samples from that population w we have different statistics\n",
    "- notation\n",
    "  - sample statistics - sample mean, sample variance, etc. Latin letters\n",
    "  - population statistics - Greek letters\n",
    "- population mean $\\mu$\n",
    "  - also called expected value\n",
    "  - N - population size\n",
    "- population variance $\\sigma^2$\n",
    "  - note how since we know the entire population, there is no estimation going on\n",
    "  - so there is N in the denominator\n",
    "- population standard error\n",
    "  - $\\sigma(x) = \\sqrt{\\sigma^2(x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Five-Number Summary\n",
    "\n",
    "- conveys similar information to a histogram\n",
    "  - how many percent of the data are less than or equal to a specified number\n",
    "    - minimum (0%); first quartile (25%); median (50%); third quartile (75%); maximum (100%)\n",
    "    - generalization: quantiles - divide the frequency distribution into equal groups\n",
    "    - 100 groups = percentiles\n",
    "- visualization: boxplot\n",
    "  - middle line - median\n",
    "  - box - quartiles\n",
    "  - whiskers - largest \"non-outliers\" - 1.5 times the interquartile range\n",
    "  - points - outliers \n",
    "![Boxplot](boxplot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moments of Distributions\n",
    "\n",
    "- $r^{th}$ central moment:\n",
    "  - defined for discrete and continuous variables\n",
    "  - measure the shape of the probability distribution\n",
    "- zeroth moment: 1 (**total probability**)\n",
    "- first moment: **arithmetic mean** $\\mu$\n",
    "- second moment: **variance** $\\sigma^2$\n",
    "- third moment: **skewness** $\\gamma$\n",
    "  - asymmetry in the distribution\n",
    "- fourth moment: **kurtosis** $\\beta$\n",
    "  - heaviness of the tails\"\n",
    "  - \"Normal\": $\\beta = 3$\n",
    "  - excess kurtosis: $\\beta - 3$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moments of the Gaussian Distribution\n",
    "\n",
    "- generalization of the binomial distribution\n",
    "- mean: $\\mu$\n",
    "- median: $\\mu$\n",
    "- mode: $\\mu$\n",
    "- variance: $\\sigma^2$\n",
    "- skewness: 0\n",
    "- excess kurtosis: 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Score\n",
    "\n",
    "- in order to compare different Gaussian distributions, we can \"normalize\" them\n",
    "- change their parameters to get a \"standard\" Gaussian distribution with $\\mu = 0$ and  $\\sigma = 0$\n",
    "- we need to \"shift\" the distribution left or right and \"squish\" or \"stretch\" to achieve the required standard deviation\n",
    "- the shift is denoted by the standard score (or z-score): $Z(x) = \\frac{x - \\mu}{\\sigma}$ \n",
    "- example: 50 student scores\n",
    "  - normal distribution, mean 60 (out of 100) and standard deviation 15\n",
    "  - how well did a student perform if they had 70 / 100?\n",
    "    - top 25% of the class\n",
    "  - what marks do the top 10% of the class have?\n",
    "    - 79 and up \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many Variables\n",
    "\n",
    "## Covariance\n",
    "- up to now, we've been looking at variables on their own\n",
    "  - but in many cases they interact with each other\n",
    "- covariance is a measure of the joint variability of two variables \n",
    "  - $cov(x, y) =\\frac{1}{n} \\sum (x_i - \\bar{x})(y_i - \\bar{y})$\n",
    "  - positive: as one variable increases, the other also increases\n",
    "  - negative: as one variable increases, the other decreases\n",
    "  - zero: the two variables don't vary together at all\n",
    "- we can see that $cov(X,X) = \\sigma^2(X)$\n",
    "- in higher dimensions, we calculate a covariance matrix\n",
    "  - the same idea: element (i,j) is equal to the covariance of the $i^{th}$ and $j^{th}$ dimensions: $A_{ij} = cov(x_i,x_j)$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "\n",
    "- like the variance, covariance is in \"weird\" units\n",
    "  - we divide by the standard deviations to normalize them standard scores (similar to z-scores)\n",
    "  - $p_i = \\frac{x_i - \\bar{x}}{S_x} \\frac{y_i - \\bar{y}}{S_y}$\n",
    "  - the mean value can be calculated as\n",
    "  - $p = \\frac{1}{n} \\sum p_i = \\frac{cov(x, y)}{S_x S_y}$\n",
    "  - this is called Pearson's correlation coefficient\n",
    "- the correlation coefficient can be in [-1; 1]\n",
    "  - high absolute value strong correlation\n",
    "  - measures the linearity of a relationship ,/ between two variables\n",
    "  - cannot express other, more complex relationships "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plots \n",
    "\n",
    "- the easiest way to see how two variables are correlated\n",
    "- two versions:\n",
    "  - \"Independent\" variable - x-axis, \"dependent\" variable - y-axis\n",
    "  - two correlated variables (we can't say which is \"independent\")\n",
    "- besides, outliers usually become easily visible\n",
    "- best practices\n",
    "  - label your axes; if needed, include a legend\n",
    "  - scale / transform the variables if needed\n",
    "    - simplifies the relationship\n",
    "  - add trendlines if needed\n",
    "    - you can also plot line charts if that's what your data suggests \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Pitfalls\n",
    "\n",
    "## Correlation Does Not Imply Causation!\n",
    "\n",
    "- if two variables are correlated, this does not mean that necessarily the first causes the second\n",
    "- example: height and weight\n",
    "  - does a greater weight cause a greater height?\n",
    "- we can still describe them\n",
    "- we can predict height from weight and vice versa\n",
    "- but that still does not say anything about one causing the other \n",
    "\n",
    "## Correlation vs. Causation \n",
    "\n",
    "- reverse causation\n",
    "  - the faster the windmills rotate, the more wind there is Windmills cause wind\n",
    "- Lurking variable\n",
    "  - the more firefighters there are to put out a fire, the greater the damage caused $\\Rightarrow$ Firefighters being present at fires, cause more damage\n",
    "- bidirectional relationship\n",
    "  - predator numbers affect prey numbers, but prey numbers (amount of food) also affect predator numbers\n",
    "- coincidence\n",
    "  - http://tylervigen.com/spurious-correlations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
